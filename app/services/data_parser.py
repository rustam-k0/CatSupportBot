import re
from datetime import datetime
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞.
logger = logging.getLogger(__name__)

# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò

def _clean_amount_string(s: str) -> float | None:
    """–û—á–∏—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É —Å —Å—É–º–º–æ–π –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤ float."""
    if not isinstance(s, str):
        return None
    try:
        # –£–¥–∞–ª—è–µ–º –≤—Å—ë, –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä, —Ç–æ—á–µ–∫ –∏ –∑–∞–ø—è—Ç—ã—Ö
        cleaned = re.sub(r'[^\d,.]', '', s)
        # –ó–∞–º–µ–Ω—è–µ–º –∑–∞–ø—è—Ç—É—é –Ω–∞ —Ç–æ—á–∫—É –¥–ª—è float
        cleaned = cleaned.replace(',', '.')
        return float(cleaned)
    except (ValueError, TypeError):
        return None


def _normalize_text_for_search(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: —É–±–∏—Ä–∞–µ—Ç –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫."""
    if not isinstance(text, str):
        return ''
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r'\n+', '\n', text)
    return text.strip()

def _clean_author_string(author: str) -> str:
    """–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –æ—á–∏—â–∞–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É –∞–≤—Ç–æ—Ä–∞ –æ—Ç "–º—É—Å–æ—Ä–∞" (—é—Ä. —Ñ–æ—Ä–º, –∫–∞–≤—ã—á–µ–∫)."""
    author = author.strip('.,;:"¬´¬ª \n\t')
    # –£–¥–∞–ª—è–µ–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ-–ø—Ä–∞–≤–æ–≤—ã–µ —Ñ–æ—Ä–º—ã
    author = re.sub(r'^(–û–û–û|–ò–ü|–ê–û|–ü–ê–û|–ó–ê–û|–û–ê–û)\s+', '', author, flags=re.IGNORECASE).strip()
    author = author.strip('"¬´¬ª')
    return author

# –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –®–ê–ë–õ–û–ù–û–í –ü–û–ò–°–ö–ê (–¶–ï–ù–¢–†–ê–õ–¨–ù–û–ï –ú–ï–°–¢–û –î–õ–Ø –ò–ó–ú–ï–ù–ï–ù–ò–ô)

def _get_date_patterns() -> list:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ —à–∞–±–ª–æ–Ω–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –î–ê–¢–´.
    """
    return [
        # TIER 0: –°–∞–º—ã–µ –Ω–∞–¥–µ–∂–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã. –ò—â–µ–º –¥–∞—Ç—É —Ä—è–¥–æ–º —Å —è–≤–Ω—ã–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –æ–ø–µ—Ä–∞—Ü–∏–∏.
        {
            'tier': 0,
            'desc': '–î–∞—Ç–∞ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –≤—Ä–µ–º–µ–Ω–µ–º (—Ç–µ–∫—Å—Ç–æ–≤–∞—è, —Å –∫–ª—é—á–æ–º)',
            'regex': r'(?:–û–ø–µ—Ä–∞—Ü–∏—è\s+—Å–æ–≤–µ—Ä—à–µ–Ω–∞|–î–∞—Ç–∞\s+–æ–ø–µ—Ä–∞—Ü–∏–∏|–¢–æ–≤–∞—Ä–Ω—ã–π\s—á–µ–∫\s.*?–∑–∞)[:\s]*(\d{1,2})\s+(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+(\d{4})',
            'type': 'textual_ru',
            'flags': re.IGNORECASE
        },
        {
            'tier': 0,
            'desc': '–î–∞—Ç–∞ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –≤—Ä–µ–º–µ–Ω–µ–º (—á–∏—Å–ª–æ–≤–∞—è, —Å –∫–ª—é—á–æ–º)',
            'regex': r'(?:–û–ø–µ—Ä–∞—Ü–∏—è\s+—Å–æ–≤–µ—Ä—à–µ–Ω–∞|–î–∞—Ç–∞\s+–æ–ø–µ—Ä–∞—Ü–∏–∏)[:\s]*(\d{2}[./-]\d{2}[./-]\d{2,4})(?:\s+–≤\s+\d{1,2}:\d{2})?',
            'type': 'numeric',
            'flags': re.IGNORECASE
        },
        # TIER 1: –î–∞—Ç—ã —Ä—è–¥–æ–º —Å –¥—Ä—É–≥–∏–º–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        {
            'tier': 1,
            'desc': '–î–∞—Ç–∞ —Ä—è–¥–æ–º —Å —Å—É–º–º–æ–π –∏–ª–∏ –ø–µ—Ä–µ–≤–æ–¥–æ–º',
            'regex': r'(?:–ü–µ—Ä–µ–≤–æ–¥|–ó–∞—á–∏—Å–ª–µ–Ω–∏–µ|–°–ø–∏—Å–∞–Ω–∏–µ)\s+–æ—Ç?\s*(\d{1,2})\s+(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+(\d{4})',
            'type': 'textual_ru',
            'flags': re.IGNORECASE
        },
        {
            'tier': 1,
            'desc': '–î–∞—Ç–∞ —Ä—è–¥–æ–º —Å —Å—É–º–º–æ–π –∏–ª–∏ –ø–µ—Ä–µ–≤–æ–¥–æ–º (—á–∏—Å–ª–æ–≤–∞—è)',
            'regex': r'(?:–ü–µ—Ä–µ–≤–æ–¥|–ó–∞—á–∏—Å–ª–µ–Ω–∏–µ|–°–ø–∏—Å–∞–Ω–∏–µ)\s+–æ—Ç?\s*(\d{2}[./-]\d{2}[./-]\d{2,4})',
            'type': 'numeric',
            'flags': re.IGNORECASE
        },
        # TIER 2: –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –¥–∞—Ç—ã —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏
        {
            'tier': 2,
            'desc': '–ê–Ω–≥–ª–∏–π—Å–∫–∞—è –¥–∞—Ç–∞ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º –∏ –≤—Ä–µ–º–µ–Ω–µ–º',
            'regex': r'(\d{1,2})\s+(January|February|March|April|May|June|July|August|September|October|November|December)\s*[‚Ä¢\-]\s*\d{1,2}:\d{2}',
            'type': 'textual_en',
            'flags': re.IGNORECASE
        },
        {
            'tier': 2,
            'desc': '–ê–Ω–≥–ª–∏–π—Å–∫–∞—è –¥–∞—Ç–∞ —Å –ø—Ä–æ–±–µ–ª–æ–º –∏ –≤—Ä–µ–º–µ–Ω–µ–º',
            'regex': r'(\d{1,2})\s+(January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2}:\d{2}',
            'type': 'textual_en',
            'flags': re.IGNORECASE
        },
        # TIER 3: –û–±—â–∏–µ —à–∞–±–ª–æ–Ω—ã (–∏—Å–∫–ª—é—á–∞—è –¥–∞—Ç—ã —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
        {
            'tier': 3,
            'desc': '–õ—é–±–∞—è –¥–∞—Ç–∞ –≤ —á–∏—Å–ª–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–î–î.–ú–ú.–ì–ì–ì–ì)',
            'regex': r'\b(\d{2}[./-]\d{2}[./-]\d{2,4})\b',
            'type': 'numeric',
            'flags': 0
        },
        {
            'tier': 4,
            'desc': '–õ—é–±–∞—è –¥–∞—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (8 –æ–∫—Ç—è–±—Ä—è 2025)',
            'regex': r'(\d{1,2})\s+(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+(\d{4})\b',
            'type': 'textual_ru',
            'flags': re.IGNORECASE
        },
        {
            'tier': 5,
            'desc': '–ê–Ω–≥–ª–∏–π—Å–∫–∞—è –¥–∞—Ç–∞ –±–µ–∑ –≤—Ä–µ–º–µ–Ω–∏',
            'regex': r'\b(\d{1,2})\s+(January|February|March|April|May|June|July|August|September|October|November|December)\b',
            'type': 'textual_en',
            'flags': re.IGNORECASE
        },
        # TIER 6: –î–∞—Ç—ã —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–Ω–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        {
            'tier': 6,
            'desc': '–î–∞—Ç–∞ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞',
            'regex': r'(?:–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ|–°–æ–∑–¥–∞–Ω–æ|–î–∞—Ç–∞\s+—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è).*?(\d{1,2})\s+(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+(\d{4})',
            'type': 'textual_ru',
            'flags': re.IGNORECASE | re.DOTALL
        },
    ]

def _get_amount_patterns() -> list:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ —à–∞–±–ª–æ–Ω–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –°–£–ú–ú–´."""
    # –£–ª—É—á—à–µ–Ω–Ω—ã–π AMOUNT_REGEX, —É—Å—Ç–æ–π—á–∏–≤—ã–π –∫ —Å–ª—É—á–∞–π–Ω—ã–º –ø—Ä–æ–±–µ–ª–∞–º –≤–Ω—É—Ç—Ä–∏ —á–∏—Å–ª–∞
    AMOUNT_REGEX = r'(\d(?:\s?\d)*(?:[,.]\d{1,2})?)'
    CURRENCY_REGEX = r'(?:–†|‚ÇΩ|—Ä—É–±\.?|RUB|P)'

    return [
        {'tier': 0, 'desc': '–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ "–ò—Ç–æ–≥–æ —Å—É–º–º–∞ —á–µ–∫–∞"', 'regex': fr'(?:–ò—Ç–æ–≥–æ\s—Å—É–º–º–∞\s—á–µ–∫–∞)\s*[:\s.]*\s*{AMOUNT_REGEX}', 'flags': re.IGNORECASE},
        {'tier': 1, 'desc': '–°—É–º–º–∞ —Å —è–≤–Ω—ã–º –∑–Ω–∞–∫–æ–º "+" –∏ —Å–∏–º–≤–æ–ª–æ–º –≤–∞–ª—é—Ç—ã', 'regex': fr'\+\s*{AMOUNT_REGEX}\s*{CURRENCY_REGEX}', 'flags': re.IGNORECASE},
        {'tier': 2, 'desc': '–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ "–°—É–º–º–∞/–ò—Ç–æ–≥–æ/–í—Å–µ–≥–æ/–î–æ–ª–≥" –∏ —á–∏—Å–ª–æ', 'regex': fr'(?:–°—É–º–º–∞|–ò—Ç–æ–≥–æ|–í—Å–µ–≥–æ|–ö\s–æ–ø–ª–∞—Ç–µ|–ü–æ–ø–æ–ª–Ω–µ–Ω–∏–µ|–ü–µ—Ä–µ–≤–æ–¥|–î–æ–ª–≥\s–ø–æ—Å–ª–µ\s–æ–ø–ª–∞—Ç—ã)\s*[:\s.]*\s*{AMOUNT_REGEX}', 'flags': re.IGNORECASE},
        {'tier': 3, 'desc': '–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–µ –í–´–®–ï —á–∏—Å–ª–∞', 'regex': fr'(?:–°—É–º–º–∞|–ò—Ç–æ–≥–æ|–í—Å–µ–≥–æ|–û–ø–µ—Ä–∞—Ü–∏—è|–°—É–º–º–∞\s–≤\s–≤–∞–ª—é—Ç–µ\s–æ–ø–µ—Ä–∞—Ü–∏–∏)\s*\n+\s*{AMOUNT_REGEX}\s*{CURRENCY_REGEX}?', 'flags': re.IGNORECASE},
        {'tier': 4, 'desc': '–ß–∏—Å–ª–æ —Å —è–≤–Ω—ã–º —Å–∏–º–≤–æ–ª–æ–º –≤–∞–ª—é—Ç—ã', 'regex': fr'\b{AMOUNT_REGEX}\s*{CURRENCY_REGEX}\b', 'flags': re.IGNORECASE},
        {'tier': 5, 'desc': '–ß–∏—Å–ª–æ —Å –∫–æ–ø–µ–π–∫–∞–º–∏ (—Ñ–æ—Ä–º–∞—Ç: 1234.56)', 'regex': r'\b(\d(?:\s?\d)*[,.]\d{2})\b', 'flags': 0},
    ]

def _get_bank_patterns() -> list:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —à–∞–±–ª–æ–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ë–ê–ù–ö–ê."""
    BANK_KEYWORDS = {
        '–¢-–ë–∞–Ω–∫': ['—Ç-–±–∞–Ω–∫', '—Ç–±–∞–Ω–∫', '—Ç–∏–Ω—å–∫–æ—Ñ—Ñ', 'tinkoff', 't-bank'],
        '–°–±–µ—Ä–±–∞–Ω–∫': ['—Å–±–µ—Ä–±–∞–Ω–∫', '—Å–±–µ—Ä', 'sber', 'sberbank'],
        '–ê–ª—å—Ñ–∞-–ë–∞–Ω–∫': ['–∞–ª—å—Ñ–∞-–±–∞–Ω–∫', '–∞–ª—å—Ñ–∞', 'alfa', 'alfabank'],
        '–í–¢–ë': ['–≤—Ç–±', 'vtb'],
        '–Ø–Ω–¥–µ–∫—Å': ['—è–Ω–¥–µ–∫—Å', 'yandex'],
    }
    patterns = []
    for bank_name, keywords in BANK_KEYWORDS.items():
        regex = r'\b(' + '|'.join(keywords) + r')\b'
        patterns.append({'tier': 1, 'desc': f'–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –¥–ª—è "{bank_name}"', 'regex': regex, 'bank_name': bank_name, 'flags': re.IGNORECASE})
    return patterns

def _get_author_patterns(transaction_type: str) -> list:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —à–∞–±–ª–æ–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ê–í–¢–û–†–ê –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏."""
    AUTHOR_NAME_REGEX = r'([–ê-–Ø–Å][–∞-—è—ëA-Za-z\s."¬´¬ª-]+?)'

    if transaction_type == 'income':
        # –õ–æ–≥–∏–∫–∞ –¥–ª—è "–ü—Ä–∏—Ö–æ–¥–∞" –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        return [
            {'tier': 1, 'desc': '–ö–ª—é—á "–û—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—å", "–ü–ª–∞—Ç–µ–ª—å—â–∏–∫", "–û—Ç –∫–æ–≥–æ"', 'regex': fr'(?:–û—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—å|–ü–ª–∞—Ç–µ–ª—å—â–∏–∫|–û—Ç\s–∫–æ–≥–æ)\s*[:\s\n]*{AUTHOR_NAME_REGEX}(?=\n|$)', 'flags': re.IGNORECASE},
            {'tier': 2, 'desc': '–ò–º—è –ø–æ—Å–ª–µ —Å–ª–æ–≤–∞ "–û–ø–∏—Å–∞–Ω–∏–µ"', 'regex': r'–û–ø–∏—Å–∞–Ω–∏–µ[\s\n]+([–ê-–Ø–Å–∞-—è—ë\s]+\s[–ê-–Ø–Å]\.)', 'flags': re.IGNORECASE},
            {'tier': 3, 'desc': '–ò–º—è —Ñ–æ—Ä–º–∞—Ç–∞ (–ò–º—è –û.) –ø–æ—Å–ª–µ —Å—Ç—Ä–æ–∫–∏ —Å —Å—É–º–º–æ–π', 'regex': r'\b(?:\d[\d\s,.]*)\s*(?:–†|‚ÇΩ|—Ä—É–±\.?|RUB|P)[\s\n]+([–ê-–Ø–Å][–∞-—è—ë]+(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+)?\s+[–ê-–Ø–Å]\.)', 'flags': re.IGNORECASE},
            {'tier': 4, 'desc': '–ê–Ω–≥–ª–∏–π—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã: "From", "Sender"', 'regex': fr'(?:From|Sender)\s*[:\s\n]*{AUTHOR_NAME_REGEX}(?=\n|$)', 'flags': re.IGNORECASE},
            {'tier': 5, 'desc': '–§–æ—Ä–º–∞—Ç "–ò–º—è –û." –∏–ª–∏ "–ò–º—è –û—Ç—á–µ—Å—Ç–≤–æ –û."', 'regex': r'\b([–ê-–Ø–Å][–∞-—è—ë]+(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+){0,2}\s+[–ê-–Ø–Å]\.)\b', 'flags': 0},
        ]
    else:  # expense
        # –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è "–†–∞—Å—Ö–æ–¥–∞"
        return [
            {'tier': 0, 'desc': '–ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –≤ –Ω–∞—á–∞–ª–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–Ω–∞–¥ –∞–¥—Ä–µ—Å–æ–º)', 'regex': r'^(.*?)\n\s*(?:–ê–¥—Ä–µ—Å\s–∫–ª–∏–Ω–∏–∫–∏|–ê–¥—Ä–µ—Å)', 'flags': re.MULTILINE},
            {'tier': 1, 'desc': '–ù–∞–∑–≤–∞–Ω–∏–µ –≤ –∫–∞–≤—ã—á–∫–∞—Ö: ¬´–û–û–û –†–æ–º–∞—à–∫–∞¬ª', 'regex': r'[¬´"]([^¬ª"]{3,})[¬ª"]', 'flags': 0},
            {'tier': 1, 'desc': '–ö–ª—é—á "–ü–æ–ª—É—á–∞—Ç–µ–ª—å", "–ü—Ä–æ–¥–∞–≤–µ—Ü"', 'regex': fr'(?:–ü–æ–ª—É—á–∞—Ç–µ–ª—å|–ü—Ä–æ–¥–∞–≤–µ—Ü|–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è)\s*[:\s\n]*{AUTHOR_NAME_REGEX}(?=\n|$)', 'flags': re.IGNORECASE},
            {'tier': 2, 'desc': '–û—Ä–≥. —Ñ–æ—Ä–º–∞: –û–û–û, –ò–ü, –ê–û', 'regex': r'\b(?:–û–û–û|–ò–ü|–ê–û|–ü–ê–û)\s+[¬´"]?([^¬ª"\n]{3,40})[¬ª"]?', 'flags': re.IGNORECASE},
        ]

def _get_comment_patterns() -> list:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —à–∞–±–ª–æ–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ö–û–ú–ú–ï–ù–¢–ê–†–ò–Ø."""
    return [{'tier': 1, 'desc': '–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º', 'regex': r'(?:–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π|–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ|–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ\s–ø–ª–∞—Ç–µ–∂–∞|Note|Comment|Description)\s*[:\s\n]*(.+?)(?=\n\n|$|\n\s*‚Äî{3,})', 'flags': re.IGNORECASE | re.DOTALL}]

def _get_procedure_patterns() -> list:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —à–∞–±–ª–æ–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ü–†–û–¶–ï–î–£–†–´/–£–°–õ–£–ì–ò (–¥–ª—è —Ä–∞—Å—Ö–æ–¥–æ–≤)."""
    # –£–ª—É—á—à–µ–Ω–Ω—ã–π —à–∞–±–ª–æ–Ω: –∏—â–µ—Ç –±–ª–æ–∫ –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Ç–∞–±–ª–∏—Ü—ã –∏ –¥–æ –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏.
    return [{'tier': 1, 'desc': '–ë–ª–æ–∫ —Ç–µ–∫—Å—Ç–∞ –º–µ–∂–¥—É –∑–∞–≥–æ–ª–æ–≤–∫–æ–º —Ç–∞–±–ª–∏—Ü—ã –∏ –∏—Ç–æ–≥–æ–≤–æ–π —Å—É–º–º–æ–π', 'regex': r'(?:–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ.*?–°—Ç-—Ç—å)\s*\n(.*?)(?=\n\s*–ò—Ç–æ–≥–æ\s—Å—É–º–º–∞\s—á–µ–∫–∞)', 'flags': re.DOTALL | re.IGNORECASE}]


# ============================================================================
# –û–°–ù–û–í–ù–´–ï –§–£–ù–ö–¶–ò–ò –ü–ê–†–°–ò–ù–ì–ê
# ============================================================================

def parse_date(text: str) -> str | None:
    """–ò—â–µ—Ç –î–ê–¢–£ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∫ –≤–∏–¥—É –î–î.–ú–ú.–ì–ì–ì–ì."""
    patterns = _get_date_patterns()
    MONTHS_RU = {'—è–Ω–≤–∞—Ä—è': '01', '—Ñ–µ–≤—Ä–∞–ª—è': '02', '–º–∞—Ä—Ç–∞': '03', '–∞–ø—Ä–µ–ª—è': '04', '–º–∞—è': '05', '–∏—é–Ω—è': '06', '–∏—é–ª—è': '07', '–∞–≤–≥—É—Å—Ç–∞': '08', '—Å–µ–Ω—Ç—è–±—Ä—è': '09', '–æ–∫—Ç—è–±—Ä—è': '10', '–Ω–æ—è–±—Ä—è': '11', '–¥–µ–∫–∞–±—Ä—è': '12'}
    MONTHS_EN = {'january': '01', 'february': '02', 'march': '03', 'april': '04', 'may': '05', 'june': '06', 'july': '07', 'august': '08', 'september': '09', 'october': '10', 'november': '11', 'december': '12'}

    found_dates = []
    
    for p in patterns:
        matches = list(re.finditer(p['regex'], text, p['flags']))
        for match in matches:
            try:
                dt_obj = None
                if p.get('type') == 'textual_ru':
                    day, month_name, year = match.groups()
                    month = MONTHS_RU.get(month_name.lower())
                    if month: 
                        dt_obj = datetime.strptime(f"{day}.{month}.{year}", "%d.%m.%Y")

                elif p.get('type') == 'textual_en':
                    day, month_name = match.groups()
                    month = MONTHS_EN.get(month_name.lower())
                    year = datetime.now().year
                    if month: 
                        dt_obj = datetime.strptime(f"{day}.{month}.{year}", "%d.%m.%Y")

                elif p.get('type') == 'numeric':
                    date_str = match.group(1).replace('/', '.').replace('-', '.')
                    parts = date_str.split('.')
                    year_format = "%Y" if len(parts[2]) == 4 else "%y"
                    dt_obj = datetime.strptime(date_str, f"%d.%m.{year_format}")

                if dt_obj:
                    normalized_date = dt_obj.strftime("%d.%m.%Y")
                    found_dates.append({
                        'date': normalized_date,
                        'tier': p['tier'],
                        'match_text': match.group(0),
                        'position': match.start()
                    })
                    logger.info(f"‚úÖ –î–∞—Ç–∞ –Ω–∞–π–¥–µ–Ω–∞ (–£—Ä–æ–≤–µ–Ω—å {p['tier']}: '{p['desc']}') | –†–µ–∑—É–ª—å—Ç–∞—Ç: {normalized_date}")
            except (ValueError, IndexError) as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç—ã: {match.group(0)} | {e}")
                continue

    if found_dates:
        best_date = min(found_dates, key=lambda x: (x['tier'], x['position']))
        logger.info(f"üéØ –í—ã–±—Ä–∞–Ω–∞ –¥–∞—Ç–∞ —Å –Ω–∞–∏–≤—ã—Å—à–∏–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º: {best_date['date']} (tier: {best_date['tier']})")
        return best_date['date']
    
    logger.warning("‚ùå –î–∞—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ç–µ–∫—Å—Ç–µ")
    return None

def parse_amount(text: str, transaction_type: str) -> float | None:
    """–ò—â–µ—Ç –°–£–ú–ú–£ –≤ —Ç–µ–∫—Å—Ç–µ, –∏—Å–ø–æ–ª—å–∑—É—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫. –í –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å –∏—â–µ—Ç –∏—Ç–æ–≥–æ–≤—É—é —Å—É–º–º—É."""
    patterns = _get_amount_patterns()
    for p in patterns:
        match = re.search(p['regex'], text, p['flags'])
        if match:
            amount_str = match.groups()[-1]
            amount = _clean_amount_string(amount_str)
            if amount and amount > 0:
                logger.info(f"‚úÖ –°—É–º–º–∞ –Ω–∞–π–¥–µ–Ω–∞ | –£—Ä–æ–≤–µ–Ω—å {p['tier']} ('{p['desc']}') | –†–µ–∑—É–ª—å—Ç–∞—Ç: {amount}")
                return amount
    logger.warning(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å—É–º–º—É (—Ç–∏–ø: {transaction_type})")
    return None

def parse_bank(text: str) -> str | None:
    """–ò—â–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –ë–ê–ù–ö–ê –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º."""
    patterns = _get_bank_patterns()
    search_text = text.lower()
    for p in patterns:
        if re.search(p['regex'], search_text, p['flags']):
            bank_name = p['bank_name']
            logger.info(f"‚úÖ –ë–∞–Ω–∫ –Ω–∞–π–¥–µ–Ω | –£—Ä–æ–≤–µ–Ω—å {p['tier']} ('{p['desc']}') | –†–µ–∑—É–ª—å—Ç–∞—Ç: {bank_name}")
            return bank_name
    logger.debug("‚ùå –ë–∞–Ω–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º")
    return None

def parse_author(text: str, transaction_type: str) -> str | None:
    """–ò—â–µ—Ç –ê–í–¢–û–†–ê —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏. –î–ª—è —Ä–∞—Å—Ö–æ–¥–æ–≤ –∏—â–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –≤–≤–µ—Ä—Ö—É —á–µ–∫–∞."""
    patterns = _get_author_patterns(transaction_type)
    STOPWORDS = ['—É–ª–∏—Ü–∞', '–º–æ—Å–∫–≤–∞', '—Ä–æ—Å—Å–∏—è', '–∫–∞—Å—Å–∏—Ä', '—á–µ–∫', '–¥–æ–∫—É–º–µ–Ω—Ç',
                 '–æ–ø–µ—Ä–∞—Ü–∏—è', '–ø–ª–∞—Ç–µ–∂', '–∫–∞—Ä—Ç–∞', '—Å—á–µ—Ç', 'transaction', '—É—Å–ø–µ—à–Ω–æ']
    for p in patterns:
        matches = list(re.finditer(p['regex'], text, p['flags']))
        for match in matches:
            author = ' '.join(filter(None, match.groups())).strip()
            author = _clean_author_string(author)
            if len(author) < 2 or len(author) > 50: continue
            if any(stop in author.lower() for stop in STOPWORDS): continue
            if re.fullmatch(r'[\d\s.,]+', author): continue
            logger.info(f"‚úÖ –ê–≤—Ç–æ—Ä –Ω–∞–π–¥–µ–Ω | –£—Ä–æ–≤–µ–Ω—å {p['tier']} ('{p['desc']}') | –†–µ–∑—É–ª—å—Ç–∞—Ç: '{author}'")
            return author
    logger.warning(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∞–≤—Ç–æ—Ä–∞ (—Ç–∏–ø: {transaction_type})")
    return None

def parse_comment(text: str) -> str | None:
    """–ò—â–µ—Ç –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ô –≤ —Ç–µ–∫—Å—Ç–µ."""
    patterns = _get_comment_patterns()
    for p in patterns:
        match = re.search(p['regex'], text, p['flags'])
        if match:
            comment = match.group(1).strip().replace('\n', ' ')
            if len(comment) > 2:
                comment = comment[:200] + '...' if len(comment) > 200 else comment
                logger.info(f"‚úÖ –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –Ω–∞–π–¥–µ–Ω | –£—Ä–æ–≤–µ–Ω—å {p['tier']} ('{p['desc']}') | –†–µ–∑—É–ª—å—Ç–∞—Ç: '{comment}'")
                return comment
    logger.debug("‚ùå –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω")
    return None

def parse_procedure(text: str) -> str | None:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è —É—Å–ª—É–≥ –∏–∑ —á–µ–∫–∞, –æ—á–∏—â–∞–µ—Ç –∏—Ö –æ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π
    –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É —á–µ—Ä–µ–∑ "; ".
    """
    patterns = _get_procedure_patterns()
    for p in patterns:
        match = re.search(p['regex'], text, p['flags'])
        if match:
            procedures_block = match.group(1).strip()
            clean_lines = []
            
            for line in procedures_block.split('\n'):
                line = line.strip()
                if not line:
                    continue
                
                # 1. –£–¥–∞–ª—è–µ–º –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ "—Ä—É–±." –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫–∏
                cleaned_line = re.sub(r'[\s\d,.]+(—Ä—É–±\.?)?$', '', line).strip()
                
                # 2. –£–¥–∞–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —É—Ç–æ—á–Ω–µ–Ω–∏—è –≤ —Å–∫–æ–±–∫–∞—Ö
                cleaned_line = re.sub(r'\s*\([^)]*\)', '', cleaned_line).strip()
                
                # 3. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫—É, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏—Ö –±—É–∫–≤ (—Ñ–∏–ª—å—Ç—Ä OCR-–º—É—Å–æ—Ä–∞)
                if len(cleaned_line) > 2 and re.search(r'[–∞-—è–ê-–Ø]', cleaned_line):
                    clean_lines.append(cleaned_line)
            
            if clean_lines:
                result = '; '.join(clean_lines)
                logger.info(f"‚úÖ –ü—Ä–æ—Ü–µ–¥—É—Ä–∞ –Ω–∞–π–¥–µ–Ω–∞ | –£—Ä–æ–≤–µ–Ω—å {p['tier']} ('{p['desc']}') | –†–µ–∑—É–ª—å—Ç–∞—Ç: '{result}'")
                return result

    logger.debug("‚ùå –ü—Ä–æ—Ü–µ–¥—É—Ä–∞/—É—Å–ª—É–≥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    return None


def parse_transaction_data(text: str, transaction_type: str) -> dict:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞."""
    logger.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥. –¢–∏–ø: {transaction_type.upper()}. –û–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤.")
    
    # –î–ª—è –ø–æ–∏—Å–∫–∞ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ —Å—Ç—Ä–æ–∫
    result = {
        "date": parse_date(text),
        "amount": parse_amount(text, transaction_type),
    }

    if transaction_type == 'income':
        normalized_text = _normalize_text_for_search(text)
        result['bank'] = parse_bank(normalized_text)
        result['author'] = parse_author(normalized_text, transaction_type)
        result['comment'] = parse_comment(normalized_text)
    else:  # expense
        result['procedure'] = parse_procedure(text)
        result['author'] = parse_author(text, transaction_type)
        result['comment'] = parse_comment(text)

    filled_fields = sum(1 for v in result.values() if v is not None)
    total_fields = len(result)
    logger.info(
        f"üìä –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. "
        f"–†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ –ø–æ–ª–µ–π: {filled_fields}/{total_fields}. "
        f"–ò—Ç–æ–≥: {result}"
    )
    return result