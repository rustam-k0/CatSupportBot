# app/services/data_parser.py

import re
from datetime import datetime
import logging

# ============================================================================
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞.
# ============================================================================
logger = logging.getLogger(__name__)

# ============================================================================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ============================================================================

def _clean_amount_string(s: str) -> float | None:
    """–û—á–∏—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É —Å —Å—É–º–º–æ–π –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤ float."""
    if not isinstance(s, str):
        return None
    try:
        cleaned = re.sub(r'[^\d,.]', '', s)
        cleaned = cleaned.replace(',', '.')
        return float(cleaned)
    except (ValueError, TypeError):
        return None


def _normalize_text_for_search(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞."""
    if not isinstance(text, str):
        return ''
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r'\n+', '\n', text)
    return text.strip()

def _clean_author_string(author: str) -> str:
    """–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –æ—á–∏—â–∞–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É –∞–≤—Ç–æ—Ä–∞ –æ—Ç –º—É—Å–æ—Ä–∞."""
    author = author.strip('.,;:"¬´¬ª \n\t')
    author = re.sub(r'^(–û–û–û|–ò–ü|–ê–û|–ü–ê–û|–ó–ê–û|–û–ê–û)\s+', '', author, flags=re.IGNORECASE).strip()
    author = author.strip('"¬´¬ª')
    return author

# ============================================================================
# –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –®–ê–ë–õ–û–ù–û–í –ü–û–ò–°–ö–ê (–¶–ï–ù–¢–†–ê–õ–¨–ù–û–ï –ú–ï–°–¢–û –î–õ–Ø –ò–ó–ú–ï–ù–ï–ù–ò–ô)
# ============================================================================

def _get_date_patterns() -> list:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —à–∞–±–ª–æ–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –î–ê–¢–´."""
    return [
        {
            'tier': 1,
            'desc': '–î–∞—Ç–∞ –≤ —á–∏—Å–ª–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–î–î.–ú–ú.–ì–ì–ì–ì)',
            'regex': r'\b(\d{2}[./-]\d{2}[./-]\d{2,4})\b',
            'type': 'numeric',
            'flags': 0
        },
        # –ù–û–í–û–ï –ü–†–ê–í–ò–õ–û: –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ "8 –æ–∫—Ç—è–±—Ä—è 2025"
        {
            'tier': 2,
            'desc': '–î–∞—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (8 –æ–∫—Ç—è–±—Ä—è 2025)',
            'regex': r'(\d{1,2})\s+(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+(\d{4})',
            'type': 'textual',
            'flags': re.IGNORECASE
        },
    ]

def _get_amount_patterns() -> list:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ —à–∞–±–ª–æ–Ω–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –°–£–ú–ú–´."""
    AMOUNT_REGEX = r'([\d\s,.]+[\d])'
    CURRENCY_REGEX = r'(?:–†|‚ÇΩ|—Ä—É–±\.?|RUB)?'
    return [
        {'tier': 1, 'desc': '–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ "–°—É–º–º–∞/–ò—Ç–æ–≥–æ/–í—Å–µ–≥–æ" –∏ —á–∏—Å–ª–æ –Ω–∞ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ', 'regex': fr'(?:–°—É–º–º–∞|–ò—Ç–æ–≥–æ|–í—Å–µ–≥–æ|–°—É–º–º–∞\s–≤\s–≤–∞–ª—é—Ç–µ\s–æ–ø–µ—Ä–∞—Ü–∏–∏)\s*[:\s.]*\s*{AMOUNT_REGEX}\s*{CURRENCY_REGEX}', 'flags': re.IGNORECASE},
        {'tier': 2, 'desc': '–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–µ –í–´–®–ï —á–∏—Å–ª–∞', 'regex': fr'(?:–°—É–º–º–∞|–ò—Ç–æ–≥–æ|–í—Å–µ–≥–æ|–û–ø–µ—Ä–∞—Ü–∏—è)\s*\n+\s*{AMOUNT_REGEX}\s*{CURRENCY_REGEX}', 'flags': re.IGNORECASE},
        {'tier': 2, 'desc': '–°–∏–Ω–æ–Ω–∏–º—ã: "–ö –æ–ø–ª–∞—Ç–µ", "–ù–∞—á–∏—Å–ª–µ–Ω–æ", "–ü–æ–ø–æ–ª–Ω–µ–Ω–∏–µ"', 'regex': fr'(?:–ö\s–æ–ø–ª–∞—Ç–µ|–ù–∞—á–∏—Å–ª–µ–Ω–æ|–°–ø–∏—Å–∞–Ω–æ|–ó–∞—á–∏—Å–ª–µ–Ω–æ|–ü–æ–ª—É—á–µ–Ω–æ|–ü–æ–ø–æ–ª–Ω–µ–Ω–∏–µ)\s*[:\s.]*\s*{AMOUNT_REGEX}', 'flags': re.IGNORECASE},
        {'tier': 3, 'desc': '–°–æ–∫—Ä–∞—â–µ–Ω–∏—è: "–°—Ç-—Ç—å", "–°—Ç–æ–∏–º-—Ç—å", "–¶–µ–Ω–∞"', 'regex': fr'(?:–°—Ç-—Ç—å|–°—Ç–æ–∏–º–æ—Å—Ç—å|–°—Ç–æ–∏–º-—Ç—å|–¶–µ–Ω–∞)\s*[:\s]*\s*{AMOUNT_REGEX}', 'flags': re.IGNORECASE},
        {'tier': 4, 'desc': '–ê–Ω–≥–ª–∏–π—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã: "Amount", "Total", "Price"', 'regex': fr'(?:Amount|Total|Sum|Price)\s*[:\s]*\s*{AMOUNT_REGEX}', 'flags': re.IGNORECASE},
        {'tier': 5, 'desc': '–ß–∏—Å–ª–æ (3+ —Ü–∏—Ñ—Ä—ã) —Å —è–≤–Ω—ã–º —Å–∏–º–≤–æ–ª–æ–º –≤–∞–ª—é—Ç—ã', 'regex': fr'([\d\s,.]' + r'{3,}' + fr')\s*(?:–†|‚ÇΩ|—Ä—É–±\.?|RUB)', 'flags': re.IGNORECASE},
        {'tier': 5, 'desc': '–ß–∏—Å–ª–æ —Å –∫–æ–ø–µ–π–∫–∞–º–∏ (—Ñ–æ—Ä–º–∞—Ç: 1234.56)', 'regex': r'\b(\d{1,3}(?:\s?\d{3})*[,.]\d{2})\b', 'flags': 0},
        {'tier': 5, 'desc': '–ë–æ–ª—å—à–æ–µ —á–∏—Å–ª–æ (>999) —Å –ø—Ä–æ–±–µ–ª–∞–º–∏-—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏', 'regex': r'\b(\d{1,3}(?:\s\d{3})+[,.]?\d*)\b', 'flags': 0},
    ]

def _get_bank_patterns() -> list:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —à–∞–±–ª–æ–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ë–ê–ù–ö–ê."""
    BANK_KEYWORDS = {
        '–¢-–ë–∞–Ω–∫': ['—Ç-–±–∞–Ω–∫', '—Ç–±–∞–Ω–∫', '—Ç–∏–Ω—å–∫–æ—Ñ—Ñ', 'tinkoff'],
        '–°–±–µ—Ä–±–∞–Ω–∫': ['—Å–±–µ—Ä–±–∞–Ω–∫', '—Å–±–µ—Ä', 'sber', 'sberbank'],
        '–ê–ª—å—Ñ–∞-–ë–∞–Ω–∫': ['–∞–ª—å—Ñ–∞-–±–∞–Ω–∫', '–∞–ª—å—Ñ–∞', 'alfa', 'alfabank'],
        '–í–¢–ë': ['–≤—Ç–±', 'vtb'],
        '–Ø–Ω–¥–µ–∫—Å': ['—è–Ω–¥–µ–∫—Å', 'yandex'],
    }
    patterns = []
    for bank_name, keywords in BANK_KEYWORDS.items():
        regex = r'\b(' + '|'.join(keywords) + r')\b'
        patterns.append({'tier': 1, 'desc': f'–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –¥–ª—è "{bank_name}"', 'regex': regex, 'bank_name': bank_name, 'flags': re.IGNORECASE})
    return patterns

def _get_author_patterns(transaction_type: str) -> list:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —à–∞–±–ª–æ–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ê–í–¢–û–†–ê –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏."""
    AUTHOR_NAME_REGEX = r'([–ê-–Ø–Å–∞-—è—ëA-Za-z\s."¬´¬ª-]+?)'
    if transaction_type == 'income':
        return [
            {'tier': 1, 'desc': '–ö–ª—é—á "–û—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—å", "–ë–∞–Ω–∫ –æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—è", "–ü–ª–∞—Ç–µ–ª—å—â–∏–∫"', 'regex': fr'(?:–û—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—å|–ë–∞–Ω–∫\s–æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—è|–ü–ª–∞—Ç–µ–ª—å—â–∏–∫)\s*[:\s]*\n?\s*{AUTHOR_NAME_REGEX}(?=\n|$)', 'flags': re.IGNORECASE},
            {'tier': 2, 'desc': '–°–∏–Ω–æ–Ω–∏–º—ã: "–û—Ç –∫–æ–≥–æ", "–ò—Å—Ç–æ—á–Ω–∏–∫"', 'regex': fr'(?:–û—Ç\s–∫–æ–≥–æ|–ò—Å—Ç–æ—á–Ω–∏–∫)\s*[:\s]*\n?\s*{AUTHOR_NAME_REGEX}(?=\n|$)', 'flags': re.IGNORECASE},
            # –ù–û–í–û–ï –ü–†–ê–í–ò–õ–û: –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è —á–µ–∫–æ–≤ –°–±–µ—Ä–±–∞–Ω–∫–∞
            {
                'tier': 2,
                'desc': '–ò–º—è –ø–æ—Å–ª–µ —Å–ª–æ–≤–∞ "–û–ø–∏—Å–∞–Ω–∏–µ"',
                'regex': r'–û–ø–∏—Å–∞–Ω–∏–µ\n\s*([–ê-–Ø–Å–∞-—è—ë\s]+\s[–ê-–Ø–Å]\.)',
                'flags': re.IGNORECASE
            },
            {'tier': 4, 'desc': '–ê–Ω–≥–ª–∏–π—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã: "From", "Sender"', 'regex': fr'(?:From|Sender)\s*[:\s]*\n?\s*{AUTHOR_NAME_REGEX}(?=\n|$)', 'flags': re.IGNORECASE},
            # –£–õ–£–ß–®–ï–ù–ù–û–ï –ü–†–ê–í–ò–õ–û: –¢–µ–ø–µ—Ä—å —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç "–ò–º—è –û. –§."
            {
                'tier': 5,
                'desc': '–§–æ—Ä–º–∞—Ç "–ò–º—è –û. –§." –∏–ª–∏ "–ò–º—è –§."',
                'regex': r'\b([–ê-–Ø–Å][–∞-—è—ë]+(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+)?\s+[–ê-–Ø–Å]\.)\b',
                'flags': 0
            },
        ]
    else:  # expense
        return [
            {'tier': 1, 'desc': '–ù–∞–∑–≤–∞–Ω–∏–µ –≤ –∫–∞–≤—ã—á–∫–∞—Ö: ¬´–û–û–û –†–æ–º–∞—à–∫–∞¬ª', 'regex': r'[¬´"]([^¬ª"]{3,})[¬ª"]', 'flags': 0},
            {'tier': 1, 'desc': '–ö–ª—é—á "–ü–æ–ª—É—á–∞—Ç–µ–ª—å", "–ü—Ä–æ–¥–∞–≤–µ—Ü", "–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è"', 'regex': fr'(?:–ü–æ–ª—É—á–∞—Ç–µ–ª—å|–ü—Ä–æ–¥–∞–≤–µ—Ü|–ü–æ—Å—Ç–∞–≤—â–∏–∫|–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è)\s*[:\s]*\n?\s*{AUTHOR_NAME_REGEX}(?=\n|$)', 'flags': re.IGNORECASE},
            {'tier': 2, 'desc': '–û—Ä–≥. —Ñ–æ—Ä–º–∞: –û–û–û, –ò–ü, –ê–û, –ü–ê–û', 'regex': r'\b(?:–û–û–û|–ò–ü|–ê–û|–ü–ê–û|–ó–ê–û|–û–ê–û)\s+[¬´"]?([^¬ª"\n]{3,40})[¬ª"]?', 'flags': re.IGNORECASE},
            {'tier': 4, 'desc': '–ê–Ω–≥–ª–∏–π—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã: "Merchant", "To"', 'regex': fr'(?:Merchant|To)\s*[:\s]*\n?\s*{AUTHOR_NAME_REGEX}(?=\n|$)', 'flags': re.IGNORECASE},
            {'tier': 5, 'desc': '–ü–µ—Ä–≤–∞—è –Ω–µ–ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞', 'regex': r'^\s*([–ê-–Ø–Å–∞-—è—ëA-Za-z\s¬´¬ª"-]{4,40}?)\s*$', 'flags': re.MULTILINE},
            {'tier': 5, 'desc': '–°–ª–æ–≤–æ –∏–∑ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤ (–±—Ä–µ–Ω–¥/—Å–µ—Ç—å)', 'regex': r'\b([–ê-–Ø–Å]{3,20})\b', 'flags': 0},
        ]

# –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ _get_..._patterns –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π...
def _get_comment_patterns() -> list:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —à–∞–±–ª–æ–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ö–û–ú–ú–ï–ù–¢–ê–†–ò–Ø."""
    return [{'tier': 1, 'desc': '–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º', 'regex': r'(?:–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π|–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ|–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ\s–ø–ª–∞—Ç–µ–∂–∞|Note|Comment|Description)\s*[:\s]*\n?\s*(.+?)(?=\n\n|$|\n\s*‚Äî{3,})', 'flags': re.IGNORECASE | re.DOTALL}]

def _get_procedure_patterns() -> list:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —à–∞–±–ª–æ–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ü–†–û–¶–ï–î–£–†–´/–£–°–õ–£–ì–ò (–¥–ª—è —Ä–∞—Å—Ö–æ–¥–æ–≤)."""
    return [{'tier': 1, 'desc': '–ë–ª–æ–∫ —Ç–µ–∫—Å—Ç–∞ –º–µ–∂–¥—É "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ" –∏ "–ò—Ç–æ–≥–æ"', 'regex': r'(?:–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ|–û–ø–∏—Å–∞–Ω–∏–µ\s—É—Å–ª—É–≥|–°–æ—Å—Ç–∞–≤\s—á–µ–∫–∞|–£—Å–ª—É–≥–∏)\s*\n(.*?)(?=\n\s*(?:–ò—Ç–æ–≥–æ|–í—Å–µ–≥–æ|–°—É–º–º–∞|–ü—Ä–æ–¥–∞–≤–µ—Ü))', 'flags': re.DOTALL | re.IGNORECASE}, {'tier': 5, 'desc': '–°—Ç—Ä–æ–∫–∞, —Å–æ–¥–µ—Ä–∂–∞—â–∞—è –±—É–∫–≤—ã, –Ω–æ –Ω–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ', 'regex': r'^\s*([–ê-–Ø–∞-—è\s]{5,50})\s*$', 'flags': re.MULTILINE | re.IGNORECASE}]

# ============================================================================
# –û–°–ù–û–í–ù–´–ï –§–£–ù–ö–¶–ò–ò –ü–ê–†–°–ò–ù–ì–ê
# ============================================================================

def parse_date(text: str) -> str | None:
    """–ò—â–µ—Ç –î–ê–¢–£ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∫ –≤–∏–¥—É –î–î.–ú–ú.–ì–ì–ì–ì."""
    patterns = _get_date_patterns()
    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –º–µ—Å—è—Ü–µ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤ —á–∏—Å–ª–æ
    MONTHS = {'—è–Ω–≤–∞—Ä—è': '01', '—Ñ–µ–≤—Ä–∞–ª—è': '02', '–º–∞—Ä—Ç–∞': '03', '–∞–ø—Ä–µ–ª—è': '04', '–º–∞—è': '05', '–∏—é–Ω—è': '06', '–∏—é–ª—è': '07', '–∞–≤–≥—É—Å—Ç–∞': '08', '—Å–µ–Ω—Ç—è–±—Ä—è': '09', '–æ–∫—Ç—è–±—Ä—è': '10', '–Ω–æ—è–±—Ä—è': '11', '–¥–µ–∫–∞–±—Ä—è': '12'}

    for p in patterns:
        match = re.search(p['regex'], text, p['flags'])
        if match:
            try:
                if p.get('type') == 'textual':
                    # –û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê: –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –¥–∞—Ç—ã
                    day, month_name, year = match.groups()
                    month = MONTHS.get(month_name.lower())
                    if not month: continue
                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞—Ç—É –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –≤–∏–¥
                    dt_obj = datetime.strptime(f"{day}.{month}.{year}", "%d.%m.%Y")
                else: # numeric
                    date_str = match.group(1).replace('/', '.').replace('-', '.')
                    parts = date_str.split('.')
                    year_format = "%Y" if len(parts[2]) == 4 else "%y"
                    dt_obj = datetime.strptime(date_str, f"%d.%m.{year_format}")

                normalized_date = dt_obj.strftime("%d.%m.%Y")
                logger.info(f"‚úÖ –î–∞—Ç–∞ –Ω–∞–π–¥–µ–Ω–∞ (–£—Ä–æ–≤–µ–Ω—å {p['tier']}: '{p['desc']}') | –†–µ–∑—É–ª—å—Ç–∞—Ç: {normalized_date}")
                return normalized_date
            except (ValueError, IndexError) as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç—ã: {match.group(0)} | {e}")
                continue
    logger.warning("‚ùå –î–∞—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ç–µ–∫—Å—Ç–µ")
    return None

# –§—É–Ω–∫—Ü–∏–∏ parse_amount, parse_bank, parse_author –∏ –¥—Ä—É–≥–∏–µ –æ—Å—Ç–∞—é—Ç—Å—è –ø–æ—á—Ç–∏ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π,
# —Ç–∞–∫ –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø–æ–∏—Å–∫–∞ —É–∂–µ –∑–∞–ª–æ–∂–µ–Ω–∞ –≤ –∏–µ—Ä–∞—Ä—Ö–∏—é —à–∞–±–ª–æ–Ω–æ–≤.

def parse_amount(text: str, transaction_type: str) -> float | None:
    """–ò—â–µ—Ç –°–£–ú–ú–£ –≤ —Ç–µ–∫—Å—Ç–µ, –∏—Å–ø–æ–ª—å–∑—É—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫."""
    patterns = _get_amount_patterns()
    for p in patterns:
        match = re.search(p['regex'], text, p['flags'])
        if match:
            amount_str = match.groups()[-1]
            amount = _clean_amount_string(amount_str)
            if amount and amount > 0:
                logger.info(f"‚úÖ –°—É–º–º–∞ –Ω–∞–π–¥–µ–Ω–∞ | –£—Ä–æ–≤–µ–Ω—å {p['tier']} ('{p['desc']}') | –†–µ–∑—É–ª—å—Ç–∞—Ç: {amount}")
                return amount
    logger.warning(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å—É–º–º—É (—Ç–∏–ø: {transaction_type})")
    return None

def parse_bank(text: str) -> str | None:
    """–ò—â–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –ë–ê–ù–ö–ê –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º."""
    patterns = _get_bank_patterns()
    search_text = text.lower()
    for p in patterns:
        if re.search(p['regex'], search_text, p['flags']):
            bank_name = p['bank_name']
            logger.info(f"‚úÖ –ë–∞–Ω–∫ –Ω–∞–π–¥–µ–Ω | –£—Ä–æ–≤–µ–Ω—å {p['tier']} ('{p['desc']}') | –†–µ–∑—É–ª—å—Ç–∞—Ç: {bank_name}")
            return bank_name
    logger.debug("‚ùå –ë–∞–Ω–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º")
    return None

def parse_author(text: str, transaction_type: str) -> str | None:
    """–ò—â–µ—Ç –ê–í–¢–û–†–ê —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏."""
    patterns = _get_author_patterns(transaction_type)
    STOPWORDS = ['—É–ª–∏—Ü–∞', '–º–æ—Å–∫–≤–∞', '—Ä–æ—Å—Å–∏—è', '–∫–∞—Å—Å–∏—Ä', '—á–µ–∫', '–¥–æ–∫—É–º–µ–Ω—Ç',
                 '–æ–ø–µ—Ä–∞—Ü–∏—è', '–ø–ª–∞—Ç–µ–∂', '–∫–∞—Ä—Ç–∞', '—Å—á–µ—Ç', 'transaction', '—É—Å–ø–µ—à–Ω–æ']
    for p in patterns:
        matches = list(re.finditer(p['regex'], text, p['flags']))
        for match in matches:
            author = ' '.join(filter(None, match.groups())).strip()
            author = _clean_author_string(author)
            if len(author) < 2 or len(author) > 50: continue
            if any(stop in author.lower() for stop in STOPWORDS): continue
            if re.fullmatch(r'[\d\s.,]+', author): continue
            logger.info(f"‚úÖ –ê–≤—Ç–æ—Ä –Ω–∞–π–¥–µ–Ω | –£—Ä–æ–≤–µ–Ω—å {p['tier']} ('{p['desc']}') | –†–µ–∑—É–ª—å—Ç–∞—Ç: '{author}'")
            return author
    logger.warning(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∞–≤—Ç–æ—Ä–∞ (—Ç–∏–ø: {transaction_type})")
    return None

def parse_comment(text: str) -> str | None:
    """–ò—â–µ—Ç –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ô –≤ —Ç–µ–∫—Å—Ç–µ."""
    patterns = _get_comment_patterns()
    for p in patterns:
        match = re.search(p['regex'], text, p['flags'])
        if match:
            comment = match.group(1).strip().replace('\n', ' ')
            if len(comment) > 2:
                comment = comment[:200] + '...' if len(comment) > 200 else comment
                logger.info(f"‚úÖ –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –Ω–∞–π–¥–µ–Ω | –£—Ä–æ–≤–µ–Ω—å {p['tier']} ('{p['desc']}') | –†–µ–∑—É–ª—å—Ç–∞—Ç: '{comment}'")
                return comment
    logger.debug("‚ùå –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω")
    return None

def parse_procedure(text: str) -> str | None:
    """–ò—â–µ—Ç –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ü–†–û–¶–ï–î–£–†–´/–£–°–õ–£–ì–ò –≤ —á–µ–∫–µ (–¥–ª—è —Ä–∞—Å—Ö–æ–¥–æ–≤)."""
    patterns = _get_procedure_patterns()
    for p in patterns:
        match = re.search(p['regex'], text, p['flags'])
        if match:
            lines = [re.sub(r'[\d,.\s]+(?:–†|‚ÇΩ|—Ä—É–±\.?)$', '', line).strip() for line in match.group(1).strip().split('\n')]
            clean_lines = [line for line in lines if len(line) > 2]
            if clean_lines:
                result = '; '.join(clean_lines)
                logger.info(f"‚úÖ –ü—Ä–æ—Ü–µ–¥—É—Ä–∞ –Ω–∞–π–¥–µ–Ω–∞ | –£—Ä–æ–≤–µ–Ω—å {p['tier']} ('{p['desc']}') | –†–µ–∑—É–ª—å—Ç–∞—Ç: '{result}'")
                return result
    logger.debug("‚ùå –ü—Ä–æ—Ü–µ–¥—É—Ä–∞/—É—Å–ª—É–≥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    return None

# ============================================================================
# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø - –¢–û–ß–ö–ê –í–•–û–î–ê
# ============================================================================

def parse_transaction_data(text: str, transaction_type: str) -> dict:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞."""
    logger.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥. –¢–∏–ø: {transaction_type.upper()}. –û–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤.")
    normalized_text = _normalize_text_for_search(text)
    
    result = {
        "date": parse_date(normalized_text),
        "amount": parse_amount(normalized_text, transaction_type),
    }

    if transaction_type == 'income':
        result['bank'] = parse_bank(normalized_text)
        result['author'] = parse_author(normalized_text, transaction_type)
        result['comment'] = parse_comment(normalized_text)
    else:  # expense
        result['procedure'] = parse_procedure(normalized_text)
        result['author'] = parse_author(normalized_text, transaction_type)
        result['comment'] = parse_comment(normalized_text)

    filled_fields = sum(1 for v in result.values() if v is not None)
    total_fields = len(result)
    logger.info(
        f"üìä –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. "
        f"–†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ –ø–æ–ª–µ–π: {filled_fields}/{total_fields}. "
        f"–ò—Ç–æ–≥: {result}"
    )
    return result